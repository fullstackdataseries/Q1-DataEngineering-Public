{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DuckDB Data Working File\n",
    "\n",
    "In this file, we will be guided along some initial work that was done, and each cell will have at least 1 fill-in-the-blank for you to reason with\n",
    "\n",
    "The end of this file will have a data model with 2 fact tables and 3 dim tables! and we will talk about business use cases for them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import duckdb\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "# Define Data Path\n",
    "try:\n",
    "    workshop_root_dir = pathlib.Path(__file__).absolute().parent\n",
    "except:\n",
    "    workshop_root_dir = pathlib.Path(pathlib.Path().absolute().parent)\n",
    "\n",
    "if os.path.exists(workshop_root_dir / \"data/transactions_multithreaded.json\"):\n",
    "    os.chdir(workshop_root_dir)\n",
    "    \n",
    "data_path = pathlib.Path(workshop_root_dir, 'data')\n",
    "\n",
    "# Read In Data\n",
    "json_data = duckdb.sql(f\"SELECT * FROM __('{data_path}/transactions_multithreaded.json')\").fetchdf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Connection to load data into DB\n",
    "con = duckdb.connect('duckdb.db', read_only=False)\n",
    "\n",
    "# Load into PyRelation\n",
    "relation = con.from_df(json_data)\n",
    "# type(relation) # duckdb.PyRelation\n",
    "\n",
    "# Create Schema for Landing Zone and Load data into Landing Zone\n",
    "con.execute(\"\"\"CREATE SCHEMA IF NOT EXISTS landing\"\"\")\n",
    "con.execute(\"\"\"CREATE TABLE IF NOT EXISTS ________ AS SELECT * FROM relation\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display as pandas df\n",
    "con.execute(\"\"\"SELECT * FROM landing.transactions _____ 10000\"\"\").fetch_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Products\n",
    "con.execute(\"\"\"SELECT ___ FROM landing.transactions LIMIT 10000\"\"\").fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unpack LIST(STRUCT[]) type in the DF\n",
    "\n",
    "# 3 things to fill in this cell #\n",
    "\n",
    "unpacked_products_df = con.execute(\"\"\"\n",
    "            \n",
    "            -- Filter to first 20k rows\n",
    "            WITH filter AS (\n",
    "            SELECT *, ___() OVER() AS row_num\n",
    "            FROM landing.transactions\n",
    "            ),\n",
    "            \n",
    "            -- Products col is of type LIST, need to extract (1-indexed)\n",
    "            extract_list AS (\n",
    "                SELECT *,\n",
    "                    ___(Products, 1) AS Products_Extract\n",
    "                FROM filter\n",
    "                WHERE row_num <= 20000\n",
    "                        )\n",
    "\n",
    "            -- Now we can unpack the STRUCT type\n",
    "            SELECT *,\n",
    "                Products_Extract.___\n",
    "            FROM extract_list\"\"\").fetch_arrow_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check it\n",
    "unpacked_products_df.to_pandas().iloc[:, ____]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can access that above object as either a Python DF or SQL Object, wow, so cool!\n",
    "import pandas.testing as pdt\n",
    "\n",
    "# SQL\n",
    "sql_pandas_df = con.execute(\"\"\"SELECT * \n",
    "           FROM unpacked_products_df\"\"\").fetch_df()\n",
    "\n",
    "# Python\n",
    "pandas_df = unpacked_products_df.to_pandas()\n",
    "\n",
    "# Test equality (using if not pdt.assert_frame_equal() because it returns None if equal)\n",
    "if not pdt.assert_frame_equal(___, ___):\n",
    "    print(\"Dataframes are equal!\")\n",
    "else:\n",
    "    print(\"Dataframes are not equal!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns\n",
    "reordered_cols = con.execute(\"\"\"SELECT Timestamp, TransactionID, UserID, SessionID,\n",
    "                    ProductID, Quantity, Price,\n",
    "                    TotalAmount, TaxAmount, ShippingAmount, Discounts, PaymentType,\n",
    "                    ShippingAddress, BillingAddress, EventType\n",
    "            FROM ___\"\"\").fetch_df()\n",
    "\n",
    "reordered_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First col I notice is a timestamp and we want to turn that to an appropriate datetime value\n",
    "# reordered_cols\n",
    "correct_time = con.execute(\"\"\"SELECT ____(Timestamp, '%Y-%m-%dT%H:%M:%S') AS Timestamp, *\n",
    "                              FROM reordered_cols\"\"\").fetch_df()\n",
    "correct_time\n",
    "# SQL doesn't have a DROP column function, so we'll just select the columns we want when we are ready\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next up I see the string column ProductID, which we want to turn into an INT by parsing all digits after the _\n",
    "\n",
    "# 2 fill in the blanks here # \n",
    "\n",
    "correct_product_id = con.execute(\"\"\"SELECT *,\n",
    "                                    CAST(\n",
    "                                        ___(\n",
    "                                            ___(ProductID, '_'),\n",
    "                                            2) \n",
    "                                    AS INT) AS ProductID_int\n",
    "            FROM correct_time\"\"\").fetch_df()\n",
    "correct_product_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We've done a lot of the cleaning we want to do, now let's think more about our data model\n",
    "\n",
    "- We have again the Product ID which in this circumstance doesn't have much information about the products available. \n",
    "- But in the real world we would have a product catalog.\n",
    "- That would provide us context to what products we are selling, and a Data Scientist might need to easier get that information back.\n",
    "- We are going to create another table called `DIM_PRODUCTS` which would represent that product catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"\"\"\n",
    "            -- Creating it in the staging schema to be available to our data model\n",
    "            CREATE SCHEMA IF NOT EXISTS staging;\n",
    "            \n",
    "            -- Creating table dim_products\n",
    "            CREATE TABLE IF NOT EXISTS staging.___ AS\n",
    "            SELECT DISTINCT(ProductID_int) AS Product_ID,\n",
    "                       'Comment about product' AS Product_Info\n",
    "                FROM correct_product_id\"\"\").fetch_df()\n",
    "\n",
    "\n",
    "con.execute(\"\"\"SELECT * FROM staging.___\"\"\").fetch_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we are following the same process for PaymentType and EventType, let's create a function to do this, but they don't have unique IDs so we add in UUID\n",
    "# This cell has 0 fill in the blanks so you can get the jist of what's going on\n",
    "\n",
    "def create_dim_table(table_name, col_name):\n",
    "    \"\"\"\n",
    "    Create a dim table with a unique ID and a comment column\n",
    "    Assuming the attribute we are creating an ID for is not already an ID.\n",
    "    \n",
    "    Example: We have Mastercard, Visa, etc for Payment Types, but there's no Payment Type Code yet\n",
    "    We will create that payment type code\n",
    "    \n",
    "    But, for PRODUCTs, they're integers, so we don't need to recreate a different ID for those products\n",
    "    \n",
    "    Args:\n",
    "        table_name (str): Name of table to create\n",
    "        col_name (str): Name of column to create\n",
    "    \"\"\"\n",
    "    con.execute(f\"\"\"\n",
    "                -- Creating it in the staging schema to be available to our data model\n",
    "                CREATE SCHEMA IF NOT EXISTS staging;\n",
    "                \n",
    "                -- Creating table dim_{table_name}\n",
    "                CREATE TABLE IF NOT EXISTS staging.{table_name} (\n",
    "                    id INT NOT NULL UNIQUE,\n",
    "                    {col_name} VARCHAR,\n",
    "                    {col_name}_Info VARCHAR\n",
    "                );\n",
    "                \n",
    "                INSERT OR REPLACE INTO staging.{table_name}\n",
    "                WITH base_table AS (\n",
    "                            SELECT DISTINCT({col_name}) AS {col_name},\n",
    "                                'Comment about {col_name}' AS {col_name}_Info\n",
    "                    FROM correct_product_id\n",
    "                    )\n",
    "                    \n",
    "                SELECT ROW_NUMBER() OVER () as id,\n",
    "                        {col_name},\n",
    "                        {col_name}_Info\n",
    "                FROM base_table\"\"\")\n",
    " \n",
    "# Payment Type   \n",
    "con.execute(\"\"\"DROP TABLE IF EXISTS staging.dim_payment_type\"\"\")\n",
    "create_dim_table('dim_payment_type', 'PaymentType')\n",
    "#con.execute(\"\"\"SELECT * FROM staging.dim_payment_type\"\"\").fetch_df()\n",
    "\n",
    "# Event Type\n",
    "con.execute(\"\"\"DROP TABLE IF EXISTS staging.dim_event_type\"\"\")\n",
    "create_dim_table('dim_event_type', 'EventType')\n",
    "con.execute(\"\"\"SELECT * FROM staging.dim_event_type\"\"\").fetch_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dispaly Table Info\n",
    "con.execute(\"\"\"SHOW ALL TABLES\"\"\").fetch_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have some tables that came from my dev, let's show another way to filter out table names we might not want\n",
    "# https://duckdb.org/docs/archive/0.9.2/sql/information_schema\n",
    "con.execute(\"\"\"\n",
    "            SELECT *\n",
    "            FROM information_schema.tables\n",
    "            WHERE table_schema IN ('landing', 'staging')\n",
    "            AND table_name NOT IN ('___', '___')\"\"\").fetch_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to our Main Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_product_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caveat about our table: We have traits that pertain to the Line Items themselves (Product ID, QTY, and Price), and then other pieces that pertain to the Order itself (Discounts, Total Price, Billing Address, Shipping Address)\n",
    "\n",
    "### So, we will split our table now into 2 different tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_filtered_out_detail = con.execute(\"\"\"\n",
    "            -- Creating our FCT_TRANSACTIONS table\n",
    "            WITH unique_transactions AS (\n",
    "               SELECT TransactionID, ROW_NUMBER() OVER (____ __ _______) AS row_number\n",
    "               FROM correct_product_id\n",
    "            ),\n",
    "            \n",
    "            final_table AS (\n",
    "            SELECT Timestamp,\n",
    "                   transactions1.TransactionID,\n",
    "                   UserID,\n",
    "                   SessionID,\n",
    "                   TotalAmount,\n",
    "                   TaxAmount,\n",
    "                   ShippingAmount,\n",
    "                   Discounts,\n",
    "                   PaymentType,\n",
    "                   ShippingAddress,\n",
    "                   BillingAddress,\n",
    "                   EventType\n",
    "                FROM correct_product_id transactions1\n",
    "                INNER JOIN unique_transactions transactions2\n",
    "                  ON transactions1.TransactionID = transactions2.TransactionID\n",
    "               WHERE transactions2.row_number = 1\n",
    "               )\n",
    "               \n",
    "             SELECT *\n",
    "             FROM final_table\"\"\").fetch_df()\n",
    "\n",
    "\n",
    "# Step 2: Create the staging.sample_fct_transactions table using the data from the DataFrame\n",
    "con.execute(\"\"\"\n",
    "            DROP TABLE staging.sample_fct_transactions\n",
    "            \"\"\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS staging.sample_fct_transactions AS\n",
    "            SELECT *\n",
    "            FROM transactions_filtered_out_detail\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish out adding codes to the transaction table, and we've got our staging table!\n",
    "# (we've left address alone for now, this could be a good exercise to do on your own)\n",
    "\n",
    "# Fill in the Aliases in the Joins #\n",
    "\n",
    "final_staging_transactions = con.execute(\"\"\"\n",
    "            with final_staging_transactions AS (\n",
    "            SELECT fct_trans.Timestamp,\n",
    "                fct_trans.TransactionID,\n",
    "                fct_trans.UserID,\n",
    "                fct_trans.SessionID,\n",
    "                fct_trans.TotalAmount,\n",
    "                fct_trans.TaxAmount,\n",
    "                fct_trans.ShippingAmount,\n",
    "                fct_trans.Discounts,\n",
    "                fct_trans.ShippingAddress,\n",
    "                fct_trans.BillingAddress,\n",
    "                dim_payment_type.id AS PaymentTypeCode,\n",
    "                dim_event_type.id AS EventTypeCode\n",
    "            FROM staging.sample_fct_transactions fct_trans\n",
    "            INNER JOIN staging.dim_payment_type dim_payment\n",
    "              ON ___.PaymentType = ___.PaymentType\n",
    "            INNER JOIN staging.dim_event_type dim_event\n",
    "                ON ___.EventType = ___.EventType\n",
    "            )\n",
    "            \n",
    "            SELECT *\n",
    "            FROM final_staging_transactions\"\"\").fetch_df()\n",
    "\n",
    "# Step 2: Create the staging.fct_transactions table using the data from the DataFrame\n",
    "con.execute(\"\"\"\n",
    "            DROP TABLE IF EXISTS staging.fct_transactions\n",
    "            \"\"\")\n",
    "            \n",
    "con.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS staging.fct_transactions AS\n",
    "            SELECT *\n",
    "            FROM final_staging_transactions\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transaction Detail Table Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the transaction detail table\n",
    "\n",
    "# Fill in after the OVER() in the ROW_NUMBER() function #\n",
    "\n",
    "transaction_detail = con.execute(\"\"\"\n",
    "            -- Creating our FCT_TRANSACTIONS table\n",
    "            WITH unique_transaction_products AS (\n",
    "               SELECT TransactionID, ProductID_int, ROW_NUMBER() over (_____ __ ______, ______) AS row_number\n",
    "               FROM correct_product_id\n",
    "            ),\n",
    "            \n",
    "            final_table AS (\n",
    "            SELECT transactions1.TransactionID,\n",
    "                     transactions1.ProductID_int AS ProductID,\n",
    "                     Quantity,\n",
    "                     Price AS UnitPrice\n",
    "                FROM correct_product_id transactions1\n",
    "                INNER JOIN unique_transaction_products transactions2\n",
    "                  ON transactions1.TransactionID = transactions2.TransactionID\n",
    "                  AND transactions1.ProductID_int = transactions2.ProductID_int\n",
    "               WHERE transactions2.row_number = 1\n",
    "               )\n",
    "               \n",
    "               SELECT *\n",
    "               FROM final_table\"\"\").fetch_df()\n",
    "\n",
    "# Step 2: Create the staging.sample_fct_transactions table using the data from the DataFrame\n",
    "con.execute(\"\"\"\n",
    "            DROP TABLE IF EXISTS staging.fct_transaction_detail\n",
    "            \"\"\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS staging.fct_transaction_detail AS\n",
    "            SELECT *\n",
    "            FROM transaction_detail\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate what tables we have in our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"\"\"SHOW ALL TABLES;\"\"\").fetch_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have some tables that came from my dev, let's show another way to filter out table names we might not want\n",
    "# https://duckdb.org/docs/archive/0.9.2/sql/information_schema\n",
    "con.execute(\"\"\"\n",
    "            SELECT *\n",
    "            FROM information_schema.tables\n",
    "            WHERE table_schema IN ('landing', 'staging')\n",
    "            AND table_name NOT IN ('sample_fct_transaction_detail', 'sample_fct_transactions')\"\"\").fetch_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See the slides from today\n",
    "- https://www.canva.com/design/DAF6qdYt1uI/EDC80b79R8FiUZ7HNi6_ig/view?utm_content=DAF6qdYt1uI&utm_campaign=designshare&utm_medium=link&utm_source=editor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fullstack_workshop_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
